{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-8bf8ae5a5303>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From c:\\users\\varun_bawa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From c:\\users\\varun_bawa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\varun_bawa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\varun_bawa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From c:\\users\\varun_bawa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\varun_bawa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#Placeholde definition\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "#Variable definition\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#implementing softmax regression model\n",
    "y = tf.matmul(x,W) + b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "\n",
    "#train the model\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "for _ in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_:batch[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9169\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(accuracy.eval(feed_dict={x: mnist.test.images, y_:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a Multilayer Convolutional Network\n",
    "#To create this model, we're going to need to create a lot of weights and biases.\n",
    "#One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#Convolution and Pooling(Using TF vanilla)\n",
    "#Some Info\n",
    "#TensorFlow also gives us a lot of flexibility in convolution and pooling operations.\n",
    "#How do we handle the boundaries? What is our stride size? In this example, we're always going to choose the vanilla version.\n",
    "#Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input.\n",
    "#Our pooling is plain old max pooling over 2x2 blocks.\n",
    "#To keep our code cleaner, let's also abstract those operations into functions.\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "#reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, \n",
    "#and the final dimension corresponding to the number of color channels.\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "#We then convolve x_image with the weight tensor, add the bias, apply the ReLU function, and finally max pool.\n",
    "#The max_pool_2x2 method will reduce the image size to 14x14.\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_conv1 = max_pool_2x2(h_conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our Second Convolutional Layer\n",
    "\n",
    "#In order to build a deep network, we stack several layers of this type.\n",
    "#The second layer will have 64 features for each 5x5 patch.\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Densely Connected Layer\n",
    "#Now that the image size has been reduced to 7x7, we add a fully-connected layer with 1024 neurons \n",
    "#to allow processing on the entire image. We reshape the tensor from the pooling layer into a batch of vectors, \n",
    "#multiply by a weight matrix, add a bias, and apply a ReLU\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropout\n",
    "#To reduce overfitting, we will apply dropout before the readout layer. \n",
    "#We create a placeholder for the probability that a neuron's output is kept during dropout.\n",
    "#This allows us to turn dropout on during training, and turn it off during testing.\n",
    "#TensorFlow's tf.nn.dropout op automatically handles scaling neuron outputs in addition to masking them, \n",
    "#so dropout just works without any additional scaling.\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Readout Layer\n",
    "#Finally, we add a layer, just like for the one layer softmax regression above.\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 0, training accuracy: 0.04\n",
      "Steps: 100, training accuracy: 0.78\n",
      "Steps: 200, training accuracy: 0.92\n",
      "Steps: 300, training accuracy: 0.96\n",
      "Steps: 400, training accuracy: 0.88\n",
      "Steps: 500, training accuracy: 0.96\n",
      "Steps: 600, training accuracy: 0.94\n",
      "Steps: 700, training accuracy: 0.94\n",
      "Steps: 800, training accuracy: 0.92\n",
      "Steps: 900, training accuracy: 0.98\n",
      "Steps: 1000, training accuracy: 0.98\n",
      "Steps: 1100, training accuracy: 0.98\n",
      "Steps: 1200, training accuracy: 0.98\n",
      "Steps: 1300, training accuracy: 1\n",
      "Steps: 1400, training accuracy: 0.98\n",
      "Steps: 1500, training accuracy: 0.96\n",
      "Steps: 1600, training accuracy: 0.94\n",
      "Steps: 1700, training accuracy: 0.96\n",
      "Steps: 1800, training accuracy: 0.98\n",
      "Steps: 1900, training accuracy: 1\n",
      "Steps: 2000, training accuracy: 1\n",
      "Steps: 2100, training accuracy: 1\n",
      "Steps: 2200, training accuracy: 0.98\n",
      "Steps: 2300, training accuracy: 1\n",
      "Steps: 2400, training accuracy: 1\n",
      "Steps: 2500, training accuracy: 0.96\n",
      "Steps: 2600, training accuracy: 0.96\n",
      "Steps: 2700, training accuracy: 1\n",
      "Steps: 2800, training accuracy: 1\n",
      "Steps: 2900, training accuracy: 0.98\n",
      "Steps: 3000, training accuracy: 1\n",
      "Steps: 3100, training accuracy: 0.98\n",
      "Steps: 3200, training accuracy: 1\n",
      "Steps: 3300, training accuracy: 0.98\n",
      "Steps: 3400, training accuracy: 0.96\n",
      "Steps: 3500, training accuracy: 1\n",
      "Steps: 3600, training accuracy: 0.9\n",
      "Steps: 3700, training accuracy: 1\n",
      "Steps: 3800, training accuracy: 1\n",
      "Steps: 3900, training accuracy: 0.98\n",
      "Steps: 4000, training accuracy: 1\n",
      "Steps: 4100, training accuracy: 1\n",
      "Steps: 4200, training accuracy: 1\n",
      "Steps: 4300, training accuracy: 1\n",
      "Steps: 4400, training accuracy: 0.98\n",
      "Steps: 4500, training accuracy: 1\n",
      "Steps: 4600, training accuracy: 0.98\n",
      "Steps: 4700, training accuracy: 1\n",
      "Steps: 4800, training accuracy: 1\n",
      "Steps: 4900, training accuracy: 0.98\n",
      "Steps: 5000, training accuracy: 1\n",
      "Steps: 5100, training accuracy: 1\n",
      "Steps: 5200, training accuracy: 0.94\n",
      "Steps: 5300, training accuracy: 1\n",
      "Steps: 5400, training accuracy: 1\n",
      "Steps: 5500, training accuracy: 1\n",
      "Steps: 5600, training accuracy: 0.98\n",
      "Steps: 5700, training accuracy: 0.98\n",
      "Steps: 5800, training accuracy: 1\n",
      "Steps: 5900, training accuracy: 0.96\n",
      "Steps: 6000, training accuracy: 1\n",
      "Steps: 6100, training accuracy: 1\n",
      "Steps: 6200, training accuracy: 0.98\n",
      "Steps: 6300, training accuracy: 1\n",
      "Steps: 6400, training accuracy: 1\n",
      "Steps: 6500, training accuracy: 1\n",
      "Steps: 6600, training accuracy: 1\n",
      "Steps: 6700, training accuracy: 0.96\n",
      "Steps: 6800, training accuracy: 1\n",
      "Steps: 6900, training accuracy: 0.96\n",
      "Steps: 7000, training accuracy: 1\n",
      "Steps: 7100, training accuracy: 1\n",
      "Steps: 7200, training accuracy: 1\n",
      "Steps: 7300, training accuracy: 1\n",
      "Steps: 7400, training accuracy: 1\n",
      "Steps: 7500, training accuracy: 1\n",
      "Steps: 7600, training accuracy: 1\n",
      "Steps: 7700, training accuracy: 1\n",
      "Steps: 7800, training accuracy: 1\n",
      "Steps: 7900, training accuracy: 1\n",
      "Steps: 8000, training accuracy: 1\n",
      "Steps: 8100, training accuracy: 1\n",
      "Steps: 8200, training accuracy: 1\n",
      "Steps: 8300, training accuracy: 1\n",
      "Steps: 8400, training accuracy: 0.98\n",
      "Steps: 8500, training accuracy: 1\n",
      "Steps: 8600, training accuracy: 1\n",
      "Steps: 8700, training accuracy: 1\n",
      "Steps: 8800, training accuracy: 1\n",
      "Steps: 8900, training accuracy: 0.98\n",
      "Steps: 9000, training accuracy: 1\n",
      "Steps: 9100, training accuracy: 1\n",
      "Steps: 9200, training accuracy: 1\n",
      "Steps: 9300, training accuracy: 0.98\n",
      "Steps: 9400, training accuracy: 1\n",
      "Steps: 9500, training accuracy: 0.98\n",
      "Steps: 9600, training accuracy: 1\n",
      "Steps: 9700, training accuracy: 1\n",
      "Steps: 9800, training accuracy: 0.98\n",
      "Steps: 9900, training accuracy: 1\n",
      "Steps: 10000, training accuracy: 0.96\n",
      "Steps: 10100, training accuracy: 1\n",
      "Steps: 10200, training accuracy: 1\n",
      "Steps: 10300, training accuracy: 1\n",
      "Steps: 10400, training accuracy: 0.98\n",
      "Steps: 10500, training accuracy: 1\n",
      "Steps: 10600, training accuracy: 1\n",
      "Steps: 10700, training accuracy: 1\n",
      "Steps: 10800, training accuracy: 1\n",
      "Steps: 10900, training accuracy: 1\n",
      "Steps: 11000, training accuracy: 1\n",
      "Steps: 11100, training accuracy: 1\n",
      "Steps: 11200, training accuracy: 1\n",
      "Steps: 11300, training accuracy: 0.98\n",
      "Steps: 11400, training accuracy: 0.96\n",
      "Steps: 11500, training accuracy: 0.98\n",
      "Steps: 11600, training accuracy: 1\n",
      "Steps: 11700, training accuracy: 1\n",
      "Steps: 11800, training accuracy: 0.98\n",
      "Steps: 11900, training accuracy: 1\n",
      "Steps: 12000, training accuracy: 1\n",
      "Steps: 12100, training accuracy: 1\n",
      "Steps: 12200, training accuracy: 1\n",
      "Steps: 12300, training accuracy: 1\n",
      "Steps: 12400, training accuracy: 1\n",
      "Steps: 12500, training accuracy: 1\n",
      "Steps: 12600, training accuracy: 1\n",
      "Steps: 12700, training accuracy: 1\n",
      "Steps: 12800, training accuracy: 1\n",
      "Steps: 12900, training accuracy: 1\n",
      "Steps: 13000, training accuracy: 1\n",
      "Steps: 13100, training accuracy: 1\n",
      "Steps: 13200, training accuracy: 1\n",
      "Steps: 13300, training accuracy: 1\n",
      "Steps: 13400, training accuracy: 1\n",
      "Steps: 13500, training accuracy: 1\n",
      "Steps: 13600, training accuracy: 1\n",
      "Steps: 13700, training accuracy: 1\n",
      "Steps: 13800, training accuracy: 1\n",
      "Steps: 13900, training accuracy: 1\n",
      "Steps: 14000, training accuracy: 1\n",
      "Steps: 14100, training accuracy: 1\n",
      "Steps: 14200, training accuracy: 1\n",
      "Steps: 14300, training accuracy: 1\n",
      "Steps: 14400, training accuracy: 1\n",
      "Steps: 14500, training accuracy: 0.98\n",
      "Steps: 14600, training accuracy: 0.98\n",
      "Steps: 14700, training accuracy: 1\n",
      "Steps: 14800, training accuracy: 1\n",
      "Steps: 14900, training accuracy: 1\n",
      "Steps: 15000, training accuracy: 1\n",
      "Steps: 15100, training accuracy: 1\n",
      "Steps: 15200, training accuracy: 1\n",
      "Steps: 15300, training accuracy: 0.98\n",
      "Steps: 15400, training accuracy: 1\n",
      "Steps: 15500, training accuracy: 1\n",
      "Steps: 15600, training accuracy: 1\n",
      "Steps: 15700, training accuracy: 1\n",
      "Steps: 15800, training accuracy: 1\n",
      "Steps: 15900, training accuracy: 1\n",
      "Steps: 16000, training accuracy: 1\n",
      "Steps: 16100, training accuracy: 1\n",
      "Steps: 16200, training accuracy: 1\n",
      "Steps: 16300, training accuracy: 1\n",
      "Steps: 16400, training accuracy: 1\n",
      "Steps: 16500, training accuracy: 1\n",
      "Steps: 16600, training accuracy: 1\n",
      "Steps: 16700, training accuracy: 1\n",
      "Steps: 16800, training accuracy: 0.98\n",
      "Steps: 16900, training accuracy: 1\n",
      "Steps: 17000, training accuracy: 1\n",
      "Steps: 17100, training accuracy: 1\n",
      "Steps: 17200, training accuracy: 1\n",
      "Steps: 17300, training accuracy: 1\n",
      "Steps: 17400, training accuracy: 1\n",
      "Steps: 17500, training accuracy: 1\n",
      "Steps: 17600, training accuracy: 1\n",
      "Steps: 17700, training accuracy: 1\n",
      "Steps: 17800, training accuracy: 1\n",
      "Steps: 17900, training accuracy: 1\n",
      "Steps: 18000, training accuracy: 1\n",
      "Steps: 18100, training accuracy: 1\n",
      "Steps: 18200, training accuracy: 1\n",
      "Steps: 18300, training accuracy: 1\n",
      "Steps: 18400, training accuracy: 1\n",
      "Steps: 18500, training accuracy: 1\n",
      "Steps: 18600, training accuracy: 1\n",
      "Steps: 18700, training accuracy: 1\n",
      "Steps: 18800, training accuracy: 1\n",
      "Steps: 18900, training accuracy: 1\n",
      "Steps: 19000, training accuracy: 1\n",
      "Steps: 19100, training accuracy: 1\n",
      "Steps: 19200, training accuracy: 1\n",
      "Steps: 19300, training accuracy: 1\n",
      "Steps: 19400, training accuracy: 1\n",
      "Steps: 19500, training accuracy: 1\n",
      "Steps: 19600, training accuracy: 1\n",
      "Steps: 19700, training accuracy: 1\n",
      "Steps: 19800, training accuracy: 1\n",
      "Steps: 19900, training accuracy: 1\n",
      "Final Test Accuracy: 0.9924\n"
     ]
    }
   ],
   "source": [
    "#Training and Evaluate the model\n",
    "#How well does this model do? To train and evaluate it we will use code that is nearly identical \n",
    "#to that for the simple one layer SoftMax network above.\n",
    "\n",
    "#The differences are that:\n",
    "\n",
    "#We will replace the steepest gradient descent optimizer with the more sophisticated ADAM optimizer.\n",
    "#We will include the additional parameter keep_prob in feed_dict to control the dropout rate.\n",
    "#We will add logging to every 100th iteration in the training process.\n",
    "\n",
    "#Feel free to go ahead and run this code, but it does 20,000 training iterations and may take a while \n",
    "#(possibly up to half an hour), depending on your processor.\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"Steps: %s, training accuracy: %g\"%(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "    \n",
    "print(\"Final Test Accuracy: %g\"%accuracy.eval(feed_dict={x: mnist.test.images, y_:mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
